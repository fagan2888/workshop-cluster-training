{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Effective Cluster Usage and Data Management \u00b6 Wednesday, March 27, 2019. 3-4 PM. Perkins. Pre-requisites \u00b6 Please perform the following steps before the workshop , in the interest of time. You can stop by my desk at any point before the workshop if you need help with these: You should have an account on the RCE Cluster . If you do not have an account, look at Accessing the RCE for instructions on how to obtain one. You should be able to login to the cluster using the graphical client instructions here If you're using Windows, download and install PuTTY If you're on a Mac, download and install Xquartz Download and install FileZilla If you can follow the instructions here , go ahead and configure FileZilla. Download and install Atom If you already know how to, go ahead and install the following packages for Atom: remote-ftp hydrogen language-stata Download and install miniconda , if you don't already have conda. Workshop Topics \u00b6 Basic cluster usage Basic terminal usage How to submit jobs on the cluster and manage them Project folder structures and data management How to achieve sanity when your project's folders and data are a mess (i.e. restructuring your project's folders) Data management for CID Planning for a new project Folder structures Best practices and documentation Tips / tricks: How to write code that other people can run How to run Python / R / Stata code on the cluster without explicitly having to open the cluster's interface Brief intro to parallel computing","title":"Home"},{"location":"#effective-cluster-usage-and-data-management","text":"Wednesday, March 27, 2019. 3-4 PM. Perkins.","title":"Effective Cluster Usage and Data Management"},{"location":"#pre-requisites","text":"Please perform the following steps before the workshop , in the interest of time. You can stop by my desk at any point before the workshop if you need help with these: You should have an account on the RCE Cluster . If you do not have an account, look at Accessing the RCE for instructions on how to obtain one. You should be able to login to the cluster using the graphical client instructions here If you're using Windows, download and install PuTTY If you're on a Mac, download and install Xquartz Download and install FileZilla If you can follow the instructions here , go ahead and configure FileZilla. Download and install Atom If you already know how to, go ahead and install the following packages for Atom: remote-ftp hydrogen language-stata Download and install miniconda , if you don't already have conda.","title":"Pre-requisites"},{"location":"#workshop-topics","text":"Basic cluster usage Basic terminal usage How to submit jobs on the cluster and manage them Project folder structures and data management How to achieve sanity when your project's folders and data are a mess (i.e. restructuring your project's folders) Data management for CID Planning for a new project Folder structures Best practices and documentation Tips / tricks: How to write code that other people can run How to run Python / R / Stata code on the cluster without explicitly having to open the cluster's interface Brief intro to parallel computing","title":"Workshop Topics"},{"location":"cluster_usage/basic_cluster_usage/","text":"Basic cluster usage \u00b6 Working with Linux \u00b6 The RCE cluster runs on Linux. This section discusses some commands that might be useful for everday cluster usage: Change directory: cd <path to directory> Create new folder in current directory: mkdir <name of new folder> List contents of current directory: ls Including hidden files: ls -al With file sizes in KB/MB/GB instead of bytes: ls -alh Create an empty file: touch <filename> Open a Vim text editor: vi <filename> (There are a bunch of vim specific commands you need to learn to use Vim) Output the contents of a text file: cat <filename> Output the first few lines of a file: head <filename> Copy file: cp <old_file_path> <new_file_path> Move file: mv <old_file_path> <new_file_path> Rename file: mv <old_file_name> <new_file_name> Remove file: rm <file_name> For more commonly used commands, here's a cheatsheet RCE-Specific Commands \u00b6 The RCE cluster uses a framework called HTCondor. The following commands can be used for any cluster that runs on HTCondor. Connect to the RCE login node: ssh <username>@rce.hmdc.harvard.edu Connect to the RCE login node with port-forwarding: ssh -L 8889:localhost:8889 <username>@rce.hmdc.harvard.edu Check the status of your jobs: condor_q -global <username> SSH to job from the login node: condor_ssh_to_job -name \"<name of machine where job is running>\" <JobID> Check available resources: rce-info.sh Submit jobs: condor_submit <submit_file_path> or condor_submit_util Remove running jobs: condor_rm -name \"<name of the machine where job is running>\" <JobID> Other Relevant Commands \u00b6 Start new tmux session: tmux new Re-attach the last tmux session: tmux a Kill all running tmux sessions: tmux kill-server Optional Tips \u00b6 Setting up SSH Key Access \u00b6 If you don't want to type your password each time you SSH (from a computer you trust, of course), set up SSH keys. Steps (for Linux and MacOS): Check for existing SSH keys If you don't have existing keys, generate a key If you have existing keys, add to ssh-agent Upload the key to the remote server, using ssh-copy-id -i ~/.ssh/id_rsa.pub <username>@rce.hmdc.harvard.edu","title":"Basic Commands"},{"location":"cluster_usage/basic_cluster_usage/#basic-cluster-usage","text":"","title":"Basic cluster usage"},{"location":"cluster_usage/basic_cluster_usage/#working-with-linux","text":"The RCE cluster runs on Linux. This section discusses some commands that might be useful for everday cluster usage: Change directory: cd <path to directory> Create new folder in current directory: mkdir <name of new folder> List contents of current directory: ls Including hidden files: ls -al With file sizes in KB/MB/GB instead of bytes: ls -alh Create an empty file: touch <filename> Open a Vim text editor: vi <filename> (There are a bunch of vim specific commands you need to learn to use Vim) Output the contents of a text file: cat <filename> Output the first few lines of a file: head <filename> Copy file: cp <old_file_path> <new_file_path> Move file: mv <old_file_path> <new_file_path> Rename file: mv <old_file_name> <new_file_name> Remove file: rm <file_name> For more commonly used commands, here's a cheatsheet","title":"Working with Linux"},{"location":"cluster_usage/basic_cluster_usage/#rce-specific-commands","text":"The RCE cluster uses a framework called HTCondor. The following commands can be used for any cluster that runs on HTCondor. Connect to the RCE login node: ssh <username>@rce.hmdc.harvard.edu Connect to the RCE login node with port-forwarding: ssh -L 8889:localhost:8889 <username>@rce.hmdc.harvard.edu Check the status of your jobs: condor_q -global <username> SSH to job from the login node: condor_ssh_to_job -name \"<name of machine where job is running>\" <JobID> Check available resources: rce-info.sh Submit jobs: condor_submit <submit_file_path> or condor_submit_util Remove running jobs: condor_rm -name \"<name of the machine where job is running>\" <JobID>","title":"RCE-Specific Commands"},{"location":"cluster_usage/basic_cluster_usage/#other-relevant-commands","text":"Start new tmux session: tmux new Re-attach the last tmux session: tmux a Kill all running tmux sessions: tmux kill-server","title":"Other Relevant Commands"},{"location":"cluster_usage/basic_cluster_usage/#optional-tips","text":"","title":"Optional Tips"},{"location":"cluster_usage/basic_cluster_usage/#setting-up-ssh-key-access","text":"If you don't want to type your password each time you SSH (from a computer you trust, of course), set up SSH keys. Steps (for Linux and MacOS): Check for existing SSH keys If you don't have existing keys, generate a key If you have existing keys, add to ssh-agent Upload the key to the remote server, using ssh-copy-id -i ~/.ssh/id_rsa.pub <username>@rce.hmdc.harvard.edu","title":"Setting up SSH Key Access"},{"location":"cluster_usage/remote_stata_options/","text":"Stata using X-forwarding \u00b6 The typical way of running Stata (or other graphical applications) remotely, this gives you the native behavior of Stata, but suffers from latency issues, which means that the screen will still be a bit stuttery Setup \u00b6 MacOS: Install Xquartz Edit ~/.ssh/config and add: 1 2 3 4 Host * XAuthLocation /opt/X11/bin/xauth ForwardX11 yes ForwardX11Trusted yes Windows: In PuTTy, enable X11 forwarding Linux: No setup required! X11 is pre-installed Running Stata \u00b6 ssh to the RCE, adding a -Y flag to the command. You can use -X (untrusted X11 forwarding) or -Y (trusted X11 forwarding, slightly smoother) -Y is less secure, so only use it for applications you recognize (such as Stata) 1 ssh -Y <username>@rce.hmdc.harvard.edu Run the following command: rce_submit.py -r -graphical -a xstata-mp For commonly used commands and introductory tutorials, refer to RCE documentation . Stata using Jupyter Notebooks \u00b6 Thanks to Kyle Barron's package stata_kernel , we can use Stata kernels for Jupyter, allowing us to run Stata remotely with low latency Setup \u00b6 SSH into the cluster, using portforwarding 1 ssh -Y -L 8889 :localhost:8889 <username>@rce.hmdc.harvard.edu Create and prepare a conda environment, and activate it with the following commands: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Navigate to a folder with >10GB available space (one of your shared_space dirs) # Ideally this path should have no spaces # Example: cd ~/shared_space/cid_saudi/shreyas/misc/envs/ cd ~/shared_space/<rest_of_path> mkdir cid_env && cd cid_env # Create conda environment in the current folder conda create --prefix = cid python = 3 # Activate conda environment (can now be done from any folder) conda activate cid # Install necessary packages ## Add conda-forge as the main channel for downloading packages (optional) conda config --add channels conda-forge ## Download required packages conda install -c conda-forge jupyterlab nodejs Configure JupyterLab for increased security 1 2 3 4 5 # Configure JupyterLab jupyter notebook --generate-config # Set jupyter notebook password for increased security (optional) jupyter notebook password # <you will be asked for a password Install STATA for Jupyter 1 2 3 4 5 ## Install stata_kernel pip install stata_kernel python -m stata_kernel.install ## Install JupyterLab Extension for Stata syntax highlighting jupyter labextension install jupyterlab-stata-highlight Submit Jupyter job 1 2 3 4 5 6 # Make a directory somewhere to house the condor scripts mkdir ~/condorscripts && cd ~/condorscripts && mkdir condorlogs # Download scripts from Github Repo wget <> # Submit condor script condor_submit jupyter.submit Use tmux to handle connection errors/closures 1 2 3 4 # Start a new tmux window tmux new # SSH to the machine running your jupyter server /bin/bash ~/condorscripts/condorsshrce.sh <username> In your browser, go to localhost:8889 , and voila!","title":"Running Stata Remotely"},{"location":"cluster_usage/remote_stata_options/#stata-using-x-forwarding","text":"The typical way of running Stata (or other graphical applications) remotely, this gives you the native behavior of Stata, but suffers from latency issues, which means that the screen will still be a bit stuttery","title":"Stata using X-forwarding"},{"location":"cluster_usage/remote_stata_options/#setup","text":"MacOS: Install Xquartz Edit ~/.ssh/config and add: 1 2 3 4 Host * XAuthLocation /opt/X11/bin/xauth ForwardX11 yes ForwardX11Trusted yes Windows: In PuTTy, enable X11 forwarding Linux: No setup required! X11 is pre-installed","title":"Setup"},{"location":"cluster_usage/remote_stata_options/#running-stata","text":"ssh to the RCE, adding a -Y flag to the command. You can use -X (untrusted X11 forwarding) or -Y (trusted X11 forwarding, slightly smoother) -Y is less secure, so only use it for applications you recognize (such as Stata) 1 ssh -Y <username>@rce.hmdc.harvard.edu Run the following command: rce_submit.py -r -graphical -a xstata-mp For commonly used commands and introductory tutorials, refer to RCE documentation .","title":"Running Stata"},{"location":"cluster_usage/remote_stata_options/#stata-using-jupyter-notebooks","text":"Thanks to Kyle Barron's package stata_kernel , we can use Stata kernels for Jupyter, allowing us to run Stata remotely with low latency","title":"Stata using Jupyter Notebooks"},{"location":"cluster_usage/remote_stata_options/#setup_1","text":"SSH into the cluster, using portforwarding 1 ssh -Y -L 8889 :localhost:8889 <username>@rce.hmdc.harvard.edu Create and prepare a conda environment, and activate it with the following commands: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Navigate to a folder with >10GB available space (one of your shared_space dirs) # Ideally this path should have no spaces # Example: cd ~/shared_space/cid_saudi/shreyas/misc/envs/ cd ~/shared_space/<rest_of_path> mkdir cid_env && cd cid_env # Create conda environment in the current folder conda create --prefix = cid python = 3 # Activate conda environment (can now be done from any folder) conda activate cid # Install necessary packages ## Add conda-forge as the main channel for downloading packages (optional) conda config --add channels conda-forge ## Download required packages conda install -c conda-forge jupyterlab nodejs Configure JupyterLab for increased security 1 2 3 4 5 # Configure JupyterLab jupyter notebook --generate-config # Set jupyter notebook password for increased security (optional) jupyter notebook password # <you will be asked for a password Install STATA for Jupyter 1 2 3 4 5 ## Install stata_kernel pip install stata_kernel python -m stata_kernel.install ## Install JupyterLab Extension for Stata syntax highlighting jupyter labextension install jupyterlab-stata-highlight Submit Jupyter job 1 2 3 4 5 6 # Make a directory somewhere to house the condor scripts mkdir ~/condorscripts && cd ~/condorscripts && mkdir condorlogs # Download scripts from Github Repo wget <> # Submit condor script condor_submit jupyter.submit Use tmux to handle connection errors/closures 1 2 3 4 # Start a new tmux window tmux new # SSH to the machine running your jupyter server /bin/bash ~/condorscripts/condorsshrce.sh <username> In your browser, go to localhost:8889 , and voila!","title":"Setup"},{"location":"data_management/data_management/","text":"Project Data Management \u00b6 Planning \u00b6 Essentials \u00b6 Adapted from MIT Libraries' slides released under a CC-BY license Backups Cluster data often backed up, but ensure backup frequency is sufficient Solution: external hard drive (local) + CrashPlan (cloud) File organization and naming Create a shared system, follow it . Consider date conventions (YYYY-MM-DD), special characters, versioning Documentation (README files) Too much documentation > not enough documentation Document your system aka provide orientation documents Template Title Source with link Added by (project user with email) Date added Description Codebook Limitations Other notes Other aspects: Licensing, citation, ethical restrictions, legal restrictions, funder requirements Data security IRB where required Don't take data off cluster, especially if sensitive Responsibility Assign explicit responsibilities within your project for data management Checklist \u00b6 At a minimum, follow the checklist available at page 17 (Appendix A) of ICPSR's booklet Consider the slightly more detailed project start-and-end checklists from MIT Tool available for Data Management Planning: DMPTool . Note that you don't have to \"submit\" the data management plan to a funder through DMPTool unless you're explicitly asked to by the funder.","title":"Overview"},{"location":"data_management/data_management/#project-data-management","text":"","title":"Project Data Management"},{"location":"data_management/data_management/#planning","text":"","title":"Planning"},{"location":"data_management/data_management/#essentials","text":"Adapted from MIT Libraries' slides released under a CC-BY license Backups Cluster data often backed up, but ensure backup frequency is sufficient Solution: external hard drive (local) + CrashPlan (cloud) File organization and naming Create a shared system, follow it . Consider date conventions (YYYY-MM-DD), special characters, versioning Documentation (README files) Too much documentation > not enough documentation Document your system aka provide orientation documents Template Title Source with link Added by (project user with email) Date added Description Codebook Limitations Other notes Other aspects: Licensing, citation, ethical restrictions, legal restrictions, funder requirements Data security IRB where required Don't take data off cluster, especially if sensitive Responsibility Assign explicit responsibilities within your project for data management","title":"Essentials"},{"location":"data_management/data_management/#checklist","text":"At a minimum, follow the checklist available at page 17 (Appendix A) of ICPSR's booklet Consider the slightly more detailed project start-and-end checklists from MIT Tool available for Data Management Planning: DMPTool . Note that you don't have to \"submit\" the data management plan to a funder through DMPTool unless you're explicitly asked to by the funder.","title":"Checklist"},{"location":"data_management/folder_structure/","text":"Folder Structure \u00b6 Overall Structure \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data/ data_1/ raw/ ---- Raw, immutable files file_1 README.md ---- Describes file_1: sources, quirks, codebook processed/ ---- Cleaned, reshaped, filtered files file_1 README.md ---- Describes file_1, the cleaned version. Include codebook if required user_1/ src/ ---- Scripts / do-files are stored here proj/ ---- Notebooks, experimental do-files stored here figs/ tables/ documents/ Example folder structure \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 data/ nightlights/ raw/ luminosity_lksjdf111.csv intermediate/ processed/ luminosity.pq social_security/ raw/ incomes_gibberish.csv intermediate/ processed/ incomes.pq dario/ .git/ .gitignore darios_personal_files.secret src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/ andres/ .git/ .gitignore src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/ Notes \u00b6 Data folders: README.md or README.txt files in every raw and processed folders should reference every file in the folders Raw files are immutable - don't touch them once you've downloaded them Intermediate files can be altered freely Processed files can be altered, as long as you first consider the repercussions on subsequent analyses Files are cleaned, reshaped, filtered from raw to intermediate (optional), and finally to processed stages. All analyses have to be based on files in the processed folder ONLY.","title":"Folder structures"},{"location":"data_management/folder_structure/#folder-structure","text":"","title":"Folder Structure"},{"location":"data_management/folder_structure/#overall-structure","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data/ data_1/ raw/ ---- Raw, immutable files file_1 README.md ---- Describes file_1: sources, quirks, codebook processed/ ---- Cleaned, reshaped, filtered files file_1 README.md ---- Describes file_1, the cleaned version. Include codebook if required user_1/ src/ ---- Scripts / do-files are stored here proj/ ---- Notebooks, experimental do-files stored here figs/ tables/ documents/","title":"Overall Structure"},{"location":"data_management/folder_structure/#example-folder-structure","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 data/ nightlights/ raw/ luminosity_lksjdf111.csv intermediate/ processed/ luminosity.pq social_security/ raw/ incomes_gibberish.csv intermediate/ processed/ incomes.pq dario/ .git/ .gitignore darios_personal_files.secret src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/ andres/ .git/ .gitignore src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/","title":"Example folder structure"},{"location":"data_management/folder_structure/#notes","text":"Data folders: README.md or README.txt files in every raw and processed folders should reference every file in the folders Raw files are immutable - don't touch them once you've downloaded them Intermediate files can be altered freely Processed files can be altered, as long as you first consider the repercussions on subsequent analyses Files are cleaned, reshaped, filtered from raw to intermediate (optional), and finally to processed stages. All analyses have to be based on files in the processed folder ONLY.","title":"Notes"}]}