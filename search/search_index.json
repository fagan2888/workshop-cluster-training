{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Effective Cluster Usage and Data Management \u00b6 Wednesday, March 27, 2019. 3-4 PM. Perkins. Pre-requisites \u00b6 Please perform the following steps before the workshop , in the interest of time. You can stop by my desk at any point before the workshop if you need help with these: You should have an account on the RCE Cluster . If you do not have an account, look at Accessing the RCE for instructions on how to obtain one. You should be able to login to the cluster using the graphical client instructions here If you're using Windows, download and install PuTTY If you're on a Mac, download and install Xquartz Download and install FileZilla If you can follow the instructions here , go ahead and configure FileZilla. Download and install Atom If you already know how to, go ahead and install the following packages for Atom: remote-ftp hydrogen language-stata Download and install miniconda , if you don't already have conda. Workshop Topics \u00b6 Basic cluster usage Basic terminal usage How to submit jobs on the cluster and manage them Project folder structures and data management How to achieve sanity when your project's folders and data are a mess (i.e. restructuring your project's folders) Data management for CID Planning for a new project Folder structures Best practices and documentation Scripts for reorganizing existing folders Tips / tricks: How to write code that other people can run How to run Python / R / Stata code on the cluster without explicitly having to open the cluster's interface Brief intro to parallel computing","title":"Home"},{"location":"#effective-cluster-usage-and-data-management","text":"Wednesday, March 27, 2019. 3-4 PM. Perkins.","title":"Effective Cluster Usage and Data Management"},{"location":"#pre-requisites","text":"Please perform the following steps before the workshop , in the interest of time. You can stop by my desk at any point before the workshop if you need help with these: You should have an account on the RCE Cluster . If you do not have an account, look at Accessing the RCE for instructions on how to obtain one. You should be able to login to the cluster using the graphical client instructions here If you're using Windows, download and install PuTTY If you're on a Mac, download and install Xquartz Download and install FileZilla If you can follow the instructions here , go ahead and configure FileZilla. Download and install Atom If you already know how to, go ahead and install the following packages for Atom: remote-ftp hydrogen language-stata Download and install miniconda , if you don't already have conda.","title":"Pre-requisites"},{"location":"#workshop-topics","text":"Basic cluster usage Basic terminal usage How to submit jobs on the cluster and manage them Project folder structures and data management How to achieve sanity when your project's folders and data are a mess (i.e. restructuring your project's folders) Data management for CID Planning for a new project Folder structures Best practices and documentation Scripts for reorganizing existing folders Tips / tricks: How to write code that other people can run How to run Python / R / Stata code on the cluster without explicitly having to open the cluster's interface Brief intro to parallel computing","title":"Workshop Topics"},{"location":"cluster_usage/basic_cluster_usage/","text":"Basic cluster usage \u00b6 Working with Linux \u00b6 The RCE cluster runs on Linux. This section discusses some commands that might be useful for everday cluster usage: Change directory: cd <path to directory> Create new folder in current directory: mkdir <name of new folder> List contents of current directory: ls Including hidden files: ls -al With file sizes in KB/MB/GB instead of bytes: ls -alh Create an empty file: touch <filename> Open a Vim text editor: vi <filename> (There are a bunch of vim specific commands you need to learn to use Vim) Output the contents of a text file: cat <filename> Output the first few lines of a file: head <filename> Copy file: cp <old_file_path> <new_file_path> Move file: mv <old_file_path> <new_file_path> Rename file: mv <old_file_name> <new_file_name> Remove file: rm <file_name> For more commonly used commands, here's a cheatsheet RCE-Specific Commands \u00b6 The RCE cluster uses a framework called HTCondor. The following commands can be used for any cluster that runs on HTCondor. Connect to the RCE login node: ssh <username>@rce.hmdc.harvard.edu Connect to the RCE login node with port-forwarding: ssh -L 8889:localhost:8889 <username>@rce.hmdc.harvard.edu Check the status of your jobs: condor_q -global <username> SSH to job from the login node: condor_ssh_to_job -name \"<name of machine where job is running>\" <JobID> Check available resources: rce-info.sh Submit jobs: condor_submit <submit_file_path> or condor_submit_util Remove running jobs: condor_rm -name \"<name of the machine where job is running>\" <JobID> Other Relevant Commands \u00b6 Start new tmux session: tmux new Re-attach the last tmux session: tmux a Kill all running tmux sessions: tmux kill-server Optional Tips \u00b6 Setting up SSH Key Access \u00b6 If you don't want to type your password each time you SSH (from a computer you trust, of course), set up SSH keys. Steps (for Linux and MacOS): Check for existing SSH keys If you don't have existing keys, generate a key If you have existing keys, add to ssh-agent Upload the key to the remote server, using ssh-copy-id -i ~/.ssh/id_rsa.pub <username>@rce.hmdc.harvard.edu","title":"Basic Commands"},{"location":"cluster_usage/basic_cluster_usage/#basic-cluster-usage","text":"","title":"Basic cluster usage"},{"location":"cluster_usage/basic_cluster_usage/#working-with-linux","text":"The RCE cluster runs on Linux. This section discusses some commands that might be useful for everday cluster usage: Change directory: cd <path to directory> Create new folder in current directory: mkdir <name of new folder> List contents of current directory: ls Including hidden files: ls -al With file sizes in KB/MB/GB instead of bytes: ls -alh Create an empty file: touch <filename> Open a Vim text editor: vi <filename> (There are a bunch of vim specific commands you need to learn to use Vim) Output the contents of a text file: cat <filename> Output the first few lines of a file: head <filename> Copy file: cp <old_file_path> <new_file_path> Move file: mv <old_file_path> <new_file_path> Rename file: mv <old_file_name> <new_file_name> Remove file: rm <file_name> For more commonly used commands, here's a cheatsheet","title":"Working with Linux"},{"location":"cluster_usage/basic_cluster_usage/#rce-specific-commands","text":"The RCE cluster uses a framework called HTCondor. The following commands can be used for any cluster that runs on HTCondor. Connect to the RCE login node: ssh <username>@rce.hmdc.harvard.edu Connect to the RCE login node with port-forwarding: ssh -L 8889:localhost:8889 <username>@rce.hmdc.harvard.edu Check the status of your jobs: condor_q -global <username> SSH to job from the login node: condor_ssh_to_job -name \"<name of machine where job is running>\" <JobID> Check available resources: rce-info.sh Submit jobs: condor_submit <submit_file_path> or condor_submit_util Remove running jobs: condor_rm -name \"<name of the machine where job is running>\" <JobID>","title":"RCE-Specific Commands"},{"location":"cluster_usage/basic_cluster_usage/#other-relevant-commands","text":"Start new tmux session: tmux new Re-attach the last tmux session: tmux a Kill all running tmux sessions: tmux kill-server","title":"Other Relevant Commands"},{"location":"cluster_usage/basic_cluster_usage/#optional-tips","text":"","title":"Optional Tips"},{"location":"cluster_usage/basic_cluster_usage/#setting-up-ssh-key-access","text":"If you don't want to type your password each time you SSH (from a computer you trust, of course), set up SSH keys. Steps (for Linux and MacOS): Check for existing SSH keys If you don't have existing keys, generate a key If you have existing keys, add to ssh-agent Upload the key to the remote server, using ssh-copy-id -i ~/.ssh/id_rsa.pub <username>@rce.hmdc.harvard.edu","title":"Setting up SSH Key Access"},{"location":"cluster_usage/remote_stata_options/","text":"Option 1: X-forwarding \u00b6 Note The typical way of running Stata (or other graphical applications) remotely, X-forwarding gives you the native behavior of Stata, but suffers from latency issues, which means that the screen will still be a bit stuttery Setup \u00b6 MacOS: Install Xquartz Edit ~/.ssh/config and add: 1 2 3 4 Host * XAuthLocation /opt/X11/bin/xauth ForwardX11 yes ForwardX11Trusted yes Windows: In PuTTy, enable X11 forwarding Linux: No setup required! X11 is pre-installed You might have to edit ~/.ssh/config to allow X11 forwarding (same as MacOS) Running Stata \u00b6 ssh to the RCE, adding a -Y flag to the command. You can use -X (untrusted X11 forwarding) or -Y (trusted X11 forwarding, slightly smoother) -Y is less secure, so only use it for applications you recognize (such as Stata) 1 ssh -Y <username>@rce.hmdc.harvard.edu Run the RCE provided convenience-command to start STATA jobs, with a graphical interface: 1 rce_submit.py -r -graphical -a xstata-mp For commonly used commands and introductory tutorials, refer to RCE documentation . Option 2: Jupyter Notebooks \u00b6 Note Thanks to Kyle Barron's package stata_kernel , we can use Stata kernels for Jupyter, allowing us to run Stata remotely with low latency. However, Jupyter notebooks are not text files, so working with them does not have the do-file editor feel that Stata users might be used to. Additionally, Jupyter notebooks are harder to manage using Git Setup \u00b6 SSH into the cluster, using portforwarding 1 ssh -Y -L 8889 :localhost:8889 <username>@rce.hmdc.harvard.edu Create and prepare a conda environment, and activate it with the following commands: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Navigate to a folder with >10GB available space (one of your shared_space dirs) # Ideally this path should have no spaces # Example: cd ~/shared_space/cid_saudi/shreyas/misc/envs/ cd ~/shared_space/<rest_of_path> mkdir cid_env && cd cid_env # Create conda environment in the current folder conda create --prefix = cid python = 3 # Activate conda environment (can now be done from any folder) conda activate cid # Install necessary packages ## Add conda-forge as the main channel for downloading packages (optional) conda config --add channels conda-forge ## Download required packages conda install -c conda-forge jupyterlab nodejs Configure JupyterLab for increased security 1 2 3 4 5 # Configure JupyterLab jupyter notebook --generate-config # Set jupyter notebook password for increased security (optional) jupyter notebook password # <you will be asked for a password Install STATA for Jupyter 1 2 3 4 5 ## Install stata_kernel pip install stata_kernel python -m stata_kernel.install ## Install JupyterLab Extension for Stata syntax highlighting jupyter labextension install jupyterlab-stata-highlight Submit Jupyter job 1 2 3 4 5 6 # Make a directory somewhere to house the condor scripts mkdir ~/condorscripts && cd ~/condorscripts && mkdir condorlogs # Download Jupyter submission script from Github Repo curl -LJO https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/jupyter.submit # Submit condor script condor_submit jupyter.submit Use tmux to handle connection errors/closures 1 2 3 4 # Start a new tmux window tmux new # SSH to the machine running your jupyter server /bin/bash ~/condorscripts/condorsshrce.sh <username> In your browser, go to localhost:8889 , and voila! Option 3: Atom + Hydrogen \u00b6 Note The text editor Atom, using the package Hydrogen, allows you to run code interactively, inspect data and plot using Jupyter kernels. This method uses stata_kernel as well. Recommended option! Provides a do-file like feel , with low latency (i.e. no stuttering). TODO Summarise documentation","title":"Running Stata Remotely"},{"location":"cluster_usage/remote_stata_options/#option-1-x-forwarding","text":"Note The typical way of running Stata (or other graphical applications) remotely, X-forwarding gives you the native behavior of Stata, but suffers from latency issues, which means that the screen will still be a bit stuttery","title":"Option 1: X-forwarding"},{"location":"cluster_usage/remote_stata_options/#setup","text":"MacOS: Install Xquartz Edit ~/.ssh/config and add: 1 2 3 4 Host * XAuthLocation /opt/X11/bin/xauth ForwardX11 yes ForwardX11Trusted yes Windows: In PuTTy, enable X11 forwarding Linux: No setup required! X11 is pre-installed You might have to edit ~/.ssh/config to allow X11 forwarding (same as MacOS)","title":"Setup"},{"location":"cluster_usage/remote_stata_options/#running-stata","text":"ssh to the RCE, adding a -Y flag to the command. You can use -X (untrusted X11 forwarding) or -Y (trusted X11 forwarding, slightly smoother) -Y is less secure, so only use it for applications you recognize (such as Stata) 1 ssh -Y <username>@rce.hmdc.harvard.edu Run the RCE provided convenience-command to start STATA jobs, with a graphical interface: 1 rce_submit.py -r -graphical -a xstata-mp For commonly used commands and introductory tutorials, refer to RCE documentation .","title":"Running Stata"},{"location":"cluster_usage/remote_stata_options/#option-2-jupyter-notebooks","text":"Note Thanks to Kyle Barron's package stata_kernel , we can use Stata kernels for Jupyter, allowing us to run Stata remotely with low latency. However, Jupyter notebooks are not text files, so working with them does not have the do-file editor feel that Stata users might be used to. Additionally, Jupyter notebooks are harder to manage using Git","title":"Option 2: Jupyter Notebooks"},{"location":"cluster_usage/remote_stata_options/#setup_1","text":"SSH into the cluster, using portforwarding 1 ssh -Y -L 8889 :localhost:8889 <username>@rce.hmdc.harvard.edu Create and prepare a conda environment, and activate it with the following commands: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Navigate to a folder with >10GB available space (one of your shared_space dirs) # Ideally this path should have no spaces # Example: cd ~/shared_space/cid_saudi/shreyas/misc/envs/ cd ~/shared_space/<rest_of_path> mkdir cid_env && cd cid_env # Create conda environment in the current folder conda create --prefix = cid python = 3 # Activate conda environment (can now be done from any folder) conda activate cid # Install necessary packages ## Add conda-forge as the main channel for downloading packages (optional) conda config --add channels conda-forge ## Download required packages conda install -c conda-forge jupyterlab nodejs Configure JupyterLab for increased security 1 2 3 4 5 # Configure JupyterLab jupyter notebook --generate-config # Set jupyter notebook password for increased security (optional) jupyter notebook password # <you will be asked for a password Install STATA for Jupyter 1 2 3 4 5 ## Install stata_kernel pip install stata_kernel python -m stata_kernel.install ## Install JupyterLab Extension for Stata syntax highlighting jupyter labextension install jupyterlab-stata-highlight Submit Jupyter job 1 2 3 4 5 6 # Make a directory somewhere to house the condor scripts mkdir ~/condorscripts && cd ~/condorscripts && mkdir condorlogs # Download Jupyter submission script from Github Repo curl -LJO https://raw.githubusercontent.com/cid-harvard/workshop-cluster-training/master/assets/condorscripts/jupyter.submit # Submit condor script condor_submit jupyter.submit Use tmux to handle connection errors/closures 1 2 3 4 # Start a new tmux window tmux new # SSH to the machine running your jupyter server /bin/bash ~/condorscripts/condorsshrce.sh <username> In your browser, go to localhost:8889 , and voila!","title":"Setup"},{"location":"cluster_usage/remote_stata_options/#option-3-atom-hydrogen","text":"Note The text editor Atom, using the package Hydrogen, allows you to run code interactively, inspect data and plot using Jupyter kernels. This method uses stata_kernel as well. Recommended option! Provides a do-file like feel , with low latency (i.e. no stuttering). TODO Summarise documentation","title":"Option 3: Atom + Hydrogen"},{"location":"data_management/data_management/","text":"Project Data Management \u00b6 Essentials 1 \u00b6 Backups Cluster data often backed up, but ensure backup frequency is sufficient Solution: external hard drive (local) + CrashPlan (cloud) File organization and naming Create a shared system, follow it . Consider date conventions (YYYY-MM-DD), special characters, versioning Documentation (README files) Too much documentation > not enough documentation Document your system aka provide orientation documents Template: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Title Data Source with link Added by (CID member's name with email) Dates: - Data time period - Date added - Date modified (if required) Description Variable descriptions (incl keys to join with other data) Limitations Other notes (such as licensing, citation, ethical restrictions, legal restrictions, funder requirements, etc.) Data security All researchers working with Human Subjects are required to get IRB ethics training IRB review where required Don't take data off cluster, especially if sensitive Responsibility Assign explicit responsibilities within your project for data management Data Management Checklist \u00b6 Stock-taking: current and future inventory Assigned responsibilities for data management Storage and backup systems in place File organization and naming systems in place Access and security guidelines in place Documentation guidelines in place Data management \"orientation packet\" in place Ethics and privacy concerns addressed Resources \u00b6 Read the detailed version of the above checklist adapted from MIT Libraries' resources Another good checklist available at page 17 (Appendix A) of ICPSR's booklet Tool available for Data Management Planning: DMPTool . Note that you don't have to \"submit\" the data management plan to a funder through DMPTool unless you're explicitly asked to by the funder. Adapted from MIT Libraries' resources on data management released under a CC-BY license \u21a9","title":"Overview"},{"location":"data_management/data_management/#project-data-management","text":"","title":"Project Data Management"},{"location":"data_management/data_management/#essentials1","text":"Backups Cluster data often backed up, but ensure backup frequency is sufficient Solution: external hard drive (local) + CrashPlan (cloud) File organization and naming Create a shared system, follow it . Consider date conventions (YYYY-MM-DD), special characters, versioning Documentation (README files) Too much documentation > not enough documentation Document your system aka provide orientation documents Template: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Title Data Source with link Added by (CID member's name with email) Dates: - Data time period - Date added - Date modified (if required) Description Variable descriptions (incl keys to join with other data) Limitations Other notes (such as licensing, citation, ethical restrictions, legal restrictions, funder requirements, etc.) Data security All researchers working with Human Subjects are required to get IRB ethics training IRB review where required Don't take data off cluster, especially if sensitive Responsibility Assign explicit responsibilities within your project for data management","title":"Essentials1"},{"location":"data_management/data_management/#data-management-checklist","text":"Stock-taking: current and future inventory Assigned responsibilities for data management Storage and backup systems in place File organization and naming systems in place Access and security guidelines in place Documentation guidelines in place Data management \"orientation packet\" in place Ethics and privacy concerns addressed","title":"Data Management Checklist"},{"location":"data_management/data_management/#resources","text":"Read the detailed version of the above checklist adapted from MIT Libraries' resources Another good checklist available at page 17 (Appendix A) of ICPSR's booklet Tool available for Data Management Planning: DMPTool . Note that you don't have to \"submit\" the data management plan to a funder through DMPTool unless you're explicitly asked to by the funder. Adapted from MIT Libraries' resources on data management released under a CC-BY license \u21a9","title":"Resources"},{"location":"data_management/folder_structure/","text":"Folder Structure \u00b6 Overall Structure \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data/ data_1/ raw/ ---- Raw, immutable files file_1 README.md ---- Describes file_1: sources, quirks, codebook processed/ ---- Cleaned, reshaped, filtered files file_1 README.md ---- Describes file_1, the cleaned version. Include codebook if required user_1/ src/ ---- Scripts / do-files are stored here proj/ ---- Notebooks, experimental do-files stored here figs/ tables/ documents/ Example folder structure \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 data/ nightlights/ raw/ luminosity_lksjdf111.csv intermediate/ processed/ luminosity.pq social_security/ raw/ incomes_gibberish.csv intermediate/ processed/ incomes.pq dario/ .git/ .gitignore darios_personal_files.secret src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/ andres/ .git/ .gitignore src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/ Notes \u00b6 Data folders: README.md or README.txt files in every raw and processed folders should reference every file in the folders Raw files are immutable - don't touch them once you've downloaded them Intermediate files can be altered freely Processed files can be altered, as long as you first consider the repercussions on subsequent analyses Files are cleaned, reshaped, filtered from raw to intermediate (optional), and finally to processed stages. All analyses have to be based on files in the processed folder ONLY.","title":"Folder structures"},{"location":"data_management/folder_structure/#folder-structure","text":"","title":"Folder Structure"},{"location":"data_management/folder_structure/#overall-structure","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data/ data_1/ raw/ ---- Raw, immutable files file_1 README.md ---- Describes file_1: sources, quirks, codebook processed/ ---- Cleaned, reshaped, filtered files file_1 README.md ---- Describes file_1, the cleaned version. Include codebook if required user_1/ src/ ---- Scripts / do-files are stored here proj/ ---- Notebooks, experimental do-files stored here figs/ tables/ documents/","title":"Overall Structure"},{"location":"data_management/folder_structure/#example-folder-structure","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 data/ nightlights/ raw/ luminosity_lksjdf111.csv intermediate/ processed/ luminosity.pq social_security/ raw/ incomes_gibberish.csv intermediate/ processed/ incomes.pq dario/ .git/ .gitignore darios_personal_files.secret src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/ andres/ .git/ .gitignore src/ preprocessing/ cleaning_master.py cleaning_nightlights.py cleaning_social_security.py commonly_used_functions.py bash_scripts.sh notebooks/ 0-run_cleaning.ipynb 1-modelling.ipynb figs/ tables/ documents/","title":"Example folder structure"},{"location":"data_management/folder_structure/#notes","text":"Data folders: README.md or README.txt files in every raw and processed folders should reference every file in the folders Raw files are immutable - don't touch them once you've downloaded them Intermediate files can be altered freely Processed files can be altered, as long as you first consider the repercussions on subsequent analyses Files are cleaned, reshaped, filtered from raw to intermediate (optional), and finally to processed stages. All analyses have to be based on files in the processed folder ONLY.","title":"Notes"}]}